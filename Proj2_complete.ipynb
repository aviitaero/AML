{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##                                   BUAN 6341.002 Project 2\n",
    "              \n",
    "   ##                         Group 2 : Sumanth Appalakutti, Jaahnavi Tiruthani                                                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal of Project is : Multi-Class Classification of The Cardiac Arrhythmia  data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing all the Packages required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Cardiac Arrhythmia Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data1=pd.read_csv('cardiac_arrhythmia.csv', header=None) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Missing Data and Replacing the Missing Values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dataset has Missing values which are represented by \"?\". We are handling the missing values by replacing them with 0's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data1.replace('?', np.NaN)\n",
    "data2\n",
    "data2[13].fillna(0, inplace=True) # by 0\n",
    "data2[10].fillna(0, inplace=True) # by 0\n",
    "data2[11].fillna(0, inplace=True) # by 0\n",
    "data2[12].fillna(0, inplace=True) # by 0\n",
    "data2[14].fillna(0, inplace=True) # by 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsetting Data into X(Tdata) and Y datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dataset with Independent Variables\n",
    "Tdata=data2.drop([279], axis=1)\n",
    "# Target variable data\n",
    "y_data = data2[279]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying the Data Set Dimensions having Target Variable and Independent Variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((452, 279), (452,))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tdata.shape, y_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting and Scaling the Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(Tdata, y_data, random_state=10, stratify=y_data)\n",
    "#scaling the training data\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Accuracy = TP + TN / (TP + TN + FP + FN)\n",
    "# Precision = TP / (TP + FP)\n",
    "# Recall = TP / (TP + FN)  Also known as sensitivity, or True Positive Rate\n",
    "# F1 = 2 * Precision * Recall / (Precision + Recall) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaahn\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:581: Warning: The least populated class in y has only 2 members, which is too few. The minimum number of groups for any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "C:\\Users\\jaahn\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 0.56363636,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ]),\n",
       " array([ 0.98412698,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ]),\n",
       " array([ 0.71676301,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ]),\n",
       " array([63,  8,  5,  4,  1,  6,  3, 14,  2,  1,  6], dtype=int64))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining the KNN Classifier\n",
    "\n",
    "knn=KNeighborsClassifier()\n",
    "k_value={'n_neighbors':range(1,10)}\n",
    "gridknn=GridSearchCV(knn,param_grid=k_value, cv=5,return_train_score=True)\n",
    "knnmodel = gridknn.fit(X_train_scaled,y_train)\n",
    "\n",
    "# Prediction on Test Data \n",
    "knnp = knnmodel.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaahn\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:581: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of groups for any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "# Defining the Decision Tree Classifier\n",
    "dt = DecisionTreeClassifier()\n",
    "param_grid = {'max_depth': [5, 10, 20, 50, 100]}\n",
    "dt_clf = GridSearchCV(dt, param_grid, cv = 5, return_train_score=True)\n",
    "dtclf = dt_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Prediction on Test Data \n",
    "dtp = dtclf.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaahn\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:581: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of groups for any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "# Defining the Logistic Regression\n",
    "l_reg = LogisticRegression()\n",
    "param_grid = {'penalty':['l1', 'l2']}\n",
    "gridlreg = GridSearchCV(l_reg , param_grid, cv=5 , return_train_score=True)\n",
    "logregclf = gridlreg.fit(X_train_scaled, y_train)\n",
    "# Prediction on Test Data \n",
    "lrp=logregclf.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaahn\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:581: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of groups for any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "# Defining the Random Forest model\n",
    "rf = RandomForestClassifier()\n",
    "param_grid = {'max_features':[5, 10, 20, 50, 100, 150, 200, 250,279]}\n",
    "rf = GridSearchCV(rf , param_grid, cv = 5 , return_train_score=True)\n",
    "rfclf = rf.fit(X_train_scaled, y_train)\n",
    "# Prediction on Test Data \n",
    "rfp=rfclf.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaahn\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:581: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of groups for any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "svc_linear_clf = LinearSVC(random_state = 10)\n",
    "param_grid = {'C':[0.01,0.1,1,3,5,10]}\n",
    "gridsvclinear=GridSearchCV(svc_linear_clf, param_grid,cv=5)\n",
    "svclinear = gridsvclinear.fit(X_train_scaled, y_train)\n",
    "# Prediction on Test Data \n",
    "svclinearp=svclinear.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVC Kernel Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaahn\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:581: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of groups for any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "svc_clf=SVC(kernel = 'linear')\n",
    "param_grid = {'C':[0.1,1,5,10,15]}\n",
    "gridsvc=GridSearchCV(svc_clf,param_grid,cv=5)\n",
    "svclm = gridsvc.fit(X_train_scaled,y_train)\n",
    "# Prediction on Test Data \n",
    "svc_linker_p=svclm.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVC RBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaahn\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:581: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of groups for any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC       \n",
    "svc_rbf_clf=SVC(kernel = 'rbf')\n",
    "param_grid = {'C':[0.1,1,5,10,15], 'gamma':[0.01, 1, 5]}\n",
    "gridsvcrbf=GridSearchCV(svc_rbf_clf,param_grid,cv=5)\n",
    "svcrbf = gridsvcrbf.fit(X_train_scaled,y_train)\n",
    "\n",
    "# Prediction on Test Data \n",
    "svcrbf_p=svcrbf.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVC Poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaahn\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:581: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of groups for any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC       \n",
    "svc_poly_clf=SVC(kernel = 'poly', degree = 3)\n",
    "param_grid = {'C':[0.1,1,5,10,15], 'gamma':[0.01, 1, 5]}\n",
    "gridsvcpoly=GridSearchCV(svc_poly_clf,param_grid,cv=5)\n",
    "svcpoly = gridsvcpoly.fit(X_train_scaled,y_train)\n",
    "\n",
    "# Prediction on Test Data \n",
    "svcpoly_p=svcpoly.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Best Score for KNN Classifier: 0.522123893805\n",
      "The Best Parameters for Decision Tree : 0.660766961652\n",
      "The Best Parameters for Logistic Regression : 0.693215339233\n",
      "The Best Parameters for Random Forest : 0.737463126844\n",
      "The Best Parameters for Linear SVM : 0.70796460177\n",
      "The Best Parameters for SVM with Linear Kernel : 0.70796460177\n",
      "The Best Parameters for SVM with RBF Kernel : 0.699115044248\n",
      "The Best Parameters for SVM with Poly Kernel : 0.660766961652\n"
     ]
    }
   ],
   "source": [
    "# Displaying the best scores of the models\n",
    "print(\"The Best Score for KNN Classifier:\", knnmodel.best_score_ )\n",
    "print(\"The Best Parameters for Decision Tree :\", dtclf.best_score_)\n",
    "print(\"The Best Parameters for Logistic Regression :\", logregclf.best_score_)\n",
    "print(\"The Best Parameters for Random Forest :\", rfclf.best_score_)\n",
    "print(\"The Best Parameters for Linear SVM :\", svclinear.best_score_)\n",
    "print(\"The Best Parameters for SVM with Linear Kernel :\", svclm.best_score_)\n",
    "print(\"The Best Parameters for SVM with RBF Kernel :\", svcrbf.best_score_)\n",
    "print(\"The Best Parameters for SVM with Poly Kernel :\", svcpoly.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying the Best Parameters for the models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Best Parameters for KNN Classifier: {'n_neighbors': 3}\n",
      "The Best Parameters for Decision Tree : {'max_depth': 10}\n",
      "The Best Parameters for Logistic Regression : {'penalty': 'l2'}\n",
      "The Best Parameters for Random Forest : {'max_features': 100}\n",
      "The Best Parameters for Linear SVM : {'C': 1}\n",
      "The Best Parameters for SVM with Linear Kernel : {'C': 1}\n",
      "The Best Parameters for SVM with RBF Kernel : {'C': 15, 'gamma': 0.01}\n",
      "The Best Parameters for SVM with Poly Kernel : {'C': 0.1, 'gamma': 1}\n"
     ]
    }
   ],
   "source": [
    "print(\"The Best Parameters for KNN Classifier:\", knnmodel.best_params_ )\n",
    "print(\"The Best Parameters for Decision Tree :\", dtclf.best_params_ )\n",
    "print(\"The Best Parameters for Logistic Regression :\", logregclf.best_params_)\n",
    "print(\"The Best Parameters for Random Forest :\", rfclf.best_params_)\n",
    "print(\"The Best Parameters for Linear SVM :\", svclinear.best_params_)\n",
    "print(\"The Best Parameters for SVM with Linear Kernel :\", svclm.best_params_)\n",
    "print(\"The Best Parameters for SVM with RBF Kernel :\", svcrbf.best_params_)\n",
    "print(\"The Best Parameters for SVM with Poly Kernel :\", svcpoly.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Displaying the Training and Testing Score of Models using Best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The definition of models with their best parameters\n",
    "knn_bp = KNeighborsClassifier(n_neighbors=3)\n",
    "lr_bp = LogisticRegression(C=2,penalty='l2',random_state=10)\n",
    "svclinear_bp=LinearSVC(C=1, random_state = 10)\n",
    "svclm_bp = SVC(kernel = 'linear', C=1,random_state=10)\n",
    "svcrbf_bp = SVC(kernel = 'rbf', C=15,gamma=0.01,random_state=10)\n",
    "svcpoly_bp = SVC(kernel = 'poly', degree = 3, C=0.1,gamma=1,random_state=10)\n",
    "dt_bp = DecisionTreeClassifier(max_depth = 10,random_state=10)\n",
    "rf_bp = RandomForestClassifier(max_features = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model details: KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=3, p=2,\n",
      "           weights='uniform')\n",
      "Training score of the model: 0.622418879056\n",
      "Testing score of the model: 0.46017699115\n",
      "\n",
      "\n",
      "Model details: LogisticRegression(C=2, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=10, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "Training score of the model: 0.666666666667\n",
      "Testing score of the model: 0.522123893805\n",
      "\n",
      "\n",
      "Model details: LinearSVC(C=1, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=10, tol=0.0001,\n",
      "     verbose=0)\n",
      "Training score of the model: 0.834808259587\n",
      "Testing score of the model: 0.424778761062\n",
      "\n",
      "\n",
      "Model details: SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=False, random_state=10, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "Training score of the model: 0.666666666667\n",
      "Testing score of the model: 0.53982300885\n",
      "\n",
      "\n",
      "Model details: SVC(C=15, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.01, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=10, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "Training score of the model: 0.563421828909\n",
      "Testing score of the model: 0.557522123894\n",
      "\n",
      "\n",
      "Model details: SVC(C=0.1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=1, kernel='poly',\n",
      "  max_iter=-1, probability=False, random_state=10, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "Training score of the model: 1.0\n",
      "Testing score of the model: 0.424778761062\n",
      "\n",
      "\n",
      "Model details: DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=10,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=10, splitter='best')\n",
      "Training score of the model: 0.817109144543\n",
      "Testing score of the model: 0.362831858407\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models = [knn_bp,lr_bp,svclinear_bp,svclm_bp,svcrbf_bp,svcpoly_bp,dt_bp,rf_bp]\n",
    "for i in range(7):\n",
    "    models[i].fit(X_train_scaled,y_train)\n",
    "    print('Model details:',models[i])\n",
    "    print('Training score of the model:',models[i].score(X_train_scaled,y_train))\n",
    "    print('Testing score of the model:',models[i].score(X_test_scaled,y_test))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=3, p=2,\n",
      "           weights='uniform')\n",
      "training score: 0.587020648968\n",
      "testing score: 0.557522123894\n",
      "\n",
      "\n",
      "model: LogisticRegression(C=2, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=10, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "training score: 0.702064896755\n",
      "testing score: 0.592920353982\n",
      "\n",
      "\n",
      "model: LinearSVC(C=1, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=10, tol=0.0001,\n",
      "     verbose=0)\n",
      "training score: 0.941002949853\n",
      "testing score: 0.699115044248\n",
      "\n",
      "\n",
      "model: SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=False, random_state=10, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "training score: 0.654867256637\n",
      "testing score: 0.610619469027\n",
      "\n",
      "\n",
      "model: SVC(C=15, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.01, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=10, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "training score: 0.59587020649\n",
      "testing score: 0.557522123894\n",
      "\n",
      "\n",
      "model: SVC(C=0.1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=1, kernel='poly',\n",
      "  max_iter=-1, probability=False, random_state=10, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "training score: 0.65191740413\n",
      "testing score: 0.628318584071\n",
      "\n",
      "\n",
      "model: DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=10,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=10, splitter='best')\n",
      "training score: 0.879056047198\n",
      "testing score: 0.725663716814\n",
      "\n",
      "\n",
      "model: RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=100, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)\n",
      "training score: 0.790560471976\n",
      "testing score: 0.628318584071\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The list of models on which Bagging is performed\n",
    "model_list = [knn_bp,lr_bp,svclinear_bp,svclm_bp,svcrbf_bp,svcpoly_bp,dt_bp,rf_bp]\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# Running Bagging Classifier on all the models and displaying the results\n",
    "\n",
    "for m in model_list:\n",
    "    bag = BaggingClassifier(m,n_estimators=100,max_samples=100,bootstrap=True,n_jobs=-1,random_state=10)\n",
    "    bag.fit(X_train_scaled,y_train)\n",
    "    print('model:',m)\n",
    "    print('training score:',bag.score(X_train_scaled,y_train))\n",
    "    print('testing score:',bag.score(X_test_scaled,y_test))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging result explanation : ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: LogisticRegression(C=2, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=10, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "training score: 0.746312684366\n",
      "testing score: 0.628318584071\n",
      "\n",
      "\n",
      "model: LinearSVC(C=1, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=10, tol=0.0001,\n",
      "     verbose=0)\n",
      "training score: 0.941002949853\n",
      "testing score: 0.699115044248\n",
      "\n",
      "\n",
      "model: SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=False, random_state=10, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "training score: 0.542772861357\n",
      "testing score: 0.53982300885\n",
      "\n",
      "\n",
      "model: SVC(C=15, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.01, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=10, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "training score: 0.542772861357\n",
      "testing score: 0.53982300885\n",
      "\n",
      "\n",
      "model: SVC(C=0.1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=1, kernel='poly',\n",
      "  max_iter=-1, probability=False, random_state=10, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "training score: 1.0\n",
      "testing score: 0.654867256637\n",
      "\n",
      "\n",
      "model: DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=10,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=10, splitter='best')\n",
      "training score: 1.0\n",
      "testing score: 0.646017699115\n",
      "\n",
      "\n",
      "model: RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=100, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)\n",
      "training score: 1.0\n",
      "testing score: 0.637168141593\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The list of models on which we are performing Adaboost\n",
    "m_list = [lr_bp,svclinear_bp,svclm_bp,svcrbf_bp,svcpoly_bp,dt_bp,rf_bp]\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Running AdaBoost on the models\n",
    "\n",
    "for model in m_list:\n",
    "    ada = AdaBoostClassifier(model,n_estimators=100,algorithm='SAMME',learning_rate=0.5,random_state=42)\n",
    "    ada.fit(X_train_scaled,y_train)\n",
    "    print('Model details:',model)\n",
    "    print('Training score of model:',ada.score(X_train_scaled,y_train))\n",
    "    print('Testing score of model:', ada.score(X_test_scaled,y_test))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth: 1\n",
      "learning rate: 0.001\n",
      "training score: 0.622418879056\n",
      "testing score: 0.575221238938\n",
      "\n",
      "\n",
      "depth: 1\n",
      "learning rate: 0.01\n",
      "training score: 0.817109144543\n",
      "testing score: 0.70796460177\n",
      "\n",
      "\n",
      "depth: 1\n",
      "learning rate: 0.1\n",
      "training score: 0.967551622419\n",
      "testing score: 0.725663716814\n",
      "\n",
      "\n",
      "depth: 1\n",
      "learning rate: 0.5\n",
      "training score: 0.994100294985\n",
      "testing score: 0.699115044248\n",
      "\n",
      "\n",
      "depth: 2\n",
      "learning rate: 0.001\n",
      "training score: 0.749262536873\n",
      "testing score: 0.601769911504\n",
      "\n",
      "\n",
      "depth: 2\n",
      "learning rate: 0.01\n",
      "training score: 0.914454277286\n",
      "testing score: 0.681415929204\n",
      "\n",
      "\n",
      "depth: 2\n",
      "learning rate: 0.1\n",
      "training score: 1.0\n",
      "testing score: 0.699115044248\n",
      "\n",
      "\n",
      "depth: 2\n",
      "learning rate: 0.5\n",
      "training score: 1.0\n",
      "testing score: 0.672566371681\n",
      "\n",
      "\n",
      "depth: 3\n",
      "learning rate: 0.001\n",
      "training score: 0.828908554572\n",
      "testing score: 0.646017699115\n",
      "\n",
      "\n",
      "depth: 3\n",
      "learning rate: 0.01\n",
      "training score: 0.952802359882\n",
      "testing score: 0.699115044248\n",
      "\n",
      "\n",
      "depth: 3\n",
      "learning rate: 0.1\n",
      "training score: 1.0\n",
      "testing score: 0.699115044248\n",
      "\n",
      "\n",
      "depth: 3\n",
      "learning rate: 0.5\n",
      "training score: 1.0\n",
      "testing score: 0.690265486726\n",
      "\n",
      "\n",
      "depth: 4\n",
      "learning rate: 0.001\n",
      "training score: 0.867256637168\n",
      "testing score: 0.619469026549\n",
      "\n",
      "\n",
      "depth: 4\n",
      "learning rate: 0.01\n",
      "training score: 0.976401179941\n",
      "testing score: 0.663716814159\n",
      "\n",
      "\n",
      "depth: 4\n",
      "learning rate: 0.1\n",
      "training score: 1.0\n",
      "testing score: 0.699115044248\n",
      "\n",
      "\n",
      "depth: 4\n",
      "learning rate: 0.5\n",
      "training score: 1.0\n",
      "testing score: 0.699115044248\n",
      "\n",
      "\n",
      "depth: 5\n",
      "learning rate: 0.001\n",
      "training score: 0.896755162242\n",
      "testing score: 0.646017699115\n",
      "\n",
      "\n",
      "depth: 5\n",
      "learning rate: 0.01\n",
      "training score: 0.988200589971\n",
      "testing score: 0.663716814159\n",
      "\n",
      "\n",
      "depth: 5\n",
      "learning rate: 0.1\n",
      "training score: 1.0\n",
      "testing score: 0.672566371681\n",
      "\n",
      "\n",
      "depth: 5\n",
      "learning rate: 0.5\n",
      "training score: 1.0\n",
      "testing score: 0.681415929204\n",
      "\n",
      "\n",
      "depth: 6\n",
      "learning rate: 0.001\n",
      "training score: 0.938053097345\n",
      "testing score: 0.663716814159\n",
      "\n",
      "\n",
      "depth: 6\n",
      "learning rate: 0.01\n",
      "training score: 0.994100294985\n",
      "testing score: 0.654867256637\n",
      "\n",
      "\n",
      "depth: 6\n",
      "learning rate: 0.1\n",
      "training score: 1.0\n",
      "testing score: 0.681415929204\n",
      "\n",
      "\n",
      "depth: 6\n",
      "learning rate: 0.5\n",
      "training score: 1.0\n",
      "testing score: 0.681415929204\n",
      "\n",
      "\n",
      "depth: 7\n",
      "learning rate: 0.001\n",
      "training score: 0.955752212389\n",
      "testing score: 0.654867256637\n",
      "\n",
      "\n",
      "depth: 7\n",
      "learning rate: 0.01\n",
      "training score: 1.0\n",
      "testing score: 0.663716814159\n",
      "\n",
      "\n",
      "depth: 7\n",
      "learning rate: 0.1\n",
      "training score: 1.0\n",
      "testing score: 0.690265486726\n",
      "\n",
      "\n",
      "depth: 7\n",
      "learning rate: 0.5\n",
      "training score: 1.0\n",
      "testing score: 0.699115044248\n",
      "\n",
      "\n",
      "depth: 8\n",
      "learning rate: 0.001\n",
      "training score: 0.958702064897\n",
      "testing score: 0.663716814159\n",
      "\n",
      "\n",
      "depth: 8\n",
      "learning rate: 0.01\n",
      "training score: 1.0\n",
      "testing score: 0.654867256637\n",
      "\n",
      "\n",
      "depth: 8\n",
      "learning rate: 0.1\n",
      "training score: 1.0\n",
      "testing score: 0.654867256637\n",
      "\n",
      "\n",
      "depth: 8\n",
      "learning rate: 0.5\n",
      "training score: 1.0\n",
      "testing score: 0.672566371681\n",
      "\n",
      "\n",
      "depth: 9\n",
      "learning rate: 0.001\n",
      "training score: 0.976401179941\n",
      "testing score: 0.663716814159\n",
      "\n",
      "\n",
      "depth: 9\n",
      "learning rate: 0.01\n",
      "training score: 1.0\n",
      "testing score: 0.654867256637\n",
      "\n",
      "\n",
      "depth: 9\n",
      "learning rate: 0.1\n",
      "training score: 1.0\n",
      "testing score: 0.681415929204\n",
      "\n",
      "\n",
      "depth: 9\n",
      "learning rate: 0.5\n",
      "training score: 1.0\n",
      "testing score: 0.716814159292\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "for depth in range(1,10):\n",
    "    for rate in (0.001,0.01,0.1,0.5):\n",
    "        Grad_Boost = GradientBoostingClassifier(max_depth=depth,learning_rate=rate,random_state=10)\n",
    "        Grad_Boost.fit(X_train_scaled,y_train)\n",
    "        print('Depth:',depth)\n",
    "        print('Learning rate:',rate)\n",
    "        print('Training score:',Grad_Boost.score(X_train_scaled,y_train))\n",
    "        print('Testing score:',Grad_Boost.score(X_test_scaled,y_test))\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Gradient Boosting Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training score: 0.828908554572\n",
      "testing score: 0.646017699115\n"
     ]
    }
   ],
   "source": [
    "# Running the Gradient Boost with Best parameter \n",
    "Grad_Boost_best = GradientBoostingClassifier(max_depth=3,learning_rate=0.001,random_state=10)\n",
    "Grad_Boost_best.fit(X_train_scaled,y_train)\n",
    "\n",
    "#Printing the Training and Testing scores of the model\n",
    "print('Training score:',Grad_Boost_best.score(X_train_scaled,y_train))\n",
    "print('Testing score:',Grad_Boost_best.score(X_test_scaled,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation for boosting results  = ????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.15346672  0.10006548  0.09420965  0.0845956   0.05312036  0.05062328\n",
      "  0.04585218  0.04113792  0.03395551  0.0291003   0.02830696  0.02249531\n",
      "  0.02190761  0.01932186  0.01731159  0.01539366  0.01445182  0.01225009\n",
      "  0.01176798  0.00914527  0.00867875  0.00766778  0.00722819  0.00669843\n",
      "  0.0063108   0.00584685  0.0054186   0.00527819  0.0049771   0.00478087\n",
      "  0.00445785  0.00419111  0.00401413  0.00373631  0.00351611  0.00327154\n",
      "  0.00305062  0.00299937  0.00286984  0.00253784  0.00243258  0.00230367\n",
      "  0.00220401  0.00216197  0.00209371  0.00198413  0.00195335  0.00172184\n",
      "  0.00159579  0.00145356]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(452, 50)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing the PCA function\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Defining the PCA\n",
    "pca = PCA(n_components = 50)\n",
    "pca_data = pca.fit_transform(Tdata)\n",
    "print(pca.explained_variance_ratio_)\n",
    "\n",
    "# Displaying the dimensions of PCA Data\n",
    "pca_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=3, p=2,\n",
      "           weights='uniform')\n",
      "training score: 0.705014749263\n",
      "testing score: 0.53982300885\n",
      "\n",
      "\n",
      "model: LogisticRegression(C=2, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=10, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "training score: 0.908554572271\n",
      "testing score: 0.628318584071\n",
      "\n",
      "\n",
      "model: LinearSVC(C=1, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=10, tol=0.0001,\n",
      "     verbose=0)\n",
      "training score: 0.654867256637\n",
      "testing score: 0.495575221239\n",
      "\n",
      "\n",
      "model: SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=False, random_state=10, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "training score: 1.0\n",
      "testing score: 0.672566371681\n",
      "\n",
      "\n",
      "model: SVC(C=15, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.01, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=10, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "training score: 1.0\n",
      "testing score: 0.557522123894\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Splitting the PCA Data into Train and Test Data\n",
    "pca_train,pca_test,y_train,y_test=train_test_split(pca_data,y_data,random_state=10)\n",
    "\n",
    "# Running the models on PCA Data\n",
    "models = [knn_bp,lr_bp,svclinear_bp,svclm_bp,svcrbf_bp,svcpoly_bp,dt_bp,rf_bp]\n",
    "for i in range(5):\n",
    "    models[i].fit(pca_train,y_train)\n",
    "    print('Model details:',models[i])\n",
    "    print('Training score of the model:',models[i].score(pca_train,y_train))\n",
    "    print('Testing score of the model:',models[i].score(pca_test,y_test))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(452, 64)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing the PCA function\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Defining the PCA\n",
    "pca1 = PCA(n_components = 0.99)\n",
    "pca_data1 = pca1.fit_transform(Tdata)\n",
    "print(pca1.n_components_)\n",
    "\n",
    "# Displaying the dimensions of PCA Data\n",
    "pca_data1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model details: KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=3, p=2,\n",
      "           weights='uniform')\n",
      "Training score of the model: 0.70796460177\n",
      "Testing score of the model: 0.53982300885\n",
      "\n",
      "\n",
      "Model details: LogisticRegression(C=2, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=10, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "Training score of the model: 0.949852507375\n",
      "Testing score of the model: 0.566371681416\n",
      "\n",
      "\n",
      "Model details: LinearSVC(C=1, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=10, tol=0.0001,\n",
      "     verbose=0)\n",
      "Training score of the model: 0.699115044248\n",
      "Testing score of the model: 0.469026548673\n",
      "\n",
      "\n",
      "Model details: SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=False, random_state=10, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "Training score of the model: 1.0\n",
      "Testing score of the model: 0.628318584071\n",
      "\n",
      "\n",
      "Model details: SVC(C=15, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.01, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=10, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "Training score of the model: 1.0\n",
      "Testing score of the model: 0.557522123894\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Splitting the PCA Data into Train and Test Data\n",
    "pca_train,pca_test,y_train,y_test=train_test_split(pca_data1,y_data,random_state=10)\n",
    "\n",
    "# Running the models on PCA Data\n",
    "models = [knn_bp,lr_bp,svclinear_bp,svclm_bp,svcrbf_bp,svcpoly_bp,dt_bp,rf_bp]\n",
    "for i in range(5):\n",
    "    models[i].fit(pca_train,y_train)\n",
    "    print('Model details:',models[i])\n",
    "    print('Training score of the model:',models[i].score(pca_train,y_train))\n",
    "    print('Testing score of the model:',models[i].score(pca_test,y_test))\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
