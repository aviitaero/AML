{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##                                   BUAN 6341.002 Project 2\n",
    "              \n",
    "   ##                         Group 2 : Sumanth Appalakutti, Jaahnavi Tiruthani                                                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal of Project is : Multi-Class Classification of The Cardiac Arrhythmia  data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing all the Packages required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Cardiac Arrhythmia Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data1=pd.read_csv('cardiac_arrhythmia.csv', header=None) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Missing Data and Replacing the Missing Values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dataset has Missing values which are represented by \"?\". We are handling the missing values by replacing them with 0's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data2 = data1.replace('?', np.NaN)\n",
    "data2\n",
    "data2[13].fillna(0, inplace=True) # by 0\n",
    "data2[10].fillna(0, inplace=True) # by 0\n",
    "data2[11].fillna(0, inplace=True) # by 0\n",
    "data2[12].fillna(0, inplace=True) # by 0\n",
    "data2[14].fillna(0, inplace=True) # by 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsetting Data into X(Tdata) and Y datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dataset with Independent Variables\n",
    "Tdata=data2.drop([279], axis=1)\n",
    "# Target variable data\n",
    "y_data = data2[279]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying the Data Set Dimensions having Target Variable and Independent Variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((452, 279), (452,))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tdata.shape, y_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting and Scaling the Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(Tdata, y_data, random_state=10, stratify=y_data)\n",
    "#scaling the training data\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Accuracy = TP + TN / (TP + TN + FP + FN)\n",
    "# Precision = TP / (TP + FP)\n",
    "# Recall = TP / (TP + FN)  Also known as sensitivity, or True Positive Rate\n",
    "# F1 = 2 * Precision * Recall / (Precision + Recall) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaahn\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:581: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of groups for any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "# Defining the KNN Classifier\n",
    "\n",
    "knn=KNeighborsClassifier()\n",
    "k_value={'n_neighbors':range(1,10)}\n",
    "gridknn=GridSearchCV(knn,param_grid=k_value, cv=5,return_train_score=True)\n",
    "knnmodel = gridknn.fit(X_train_scaled,y_train)\n",
    "\n",
    "# Prediction on Test Data \n",
    "knnp = knnmodel.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaahn\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:581: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of groups for any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "# Defining the Decision Tree Classifier\n",
    "dt = DecisionTreeClassifier()\n",
    "param_grid = {'max_depth': [5, 10, 20, 50, 100]}\n",
    "dt_clf = GridSearchCV(dt, param_grid, cv = 5, return_train_score=True)\n",
    "dtclf = dt_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Prediction on Test Data \n",
    "dtp = dtclf.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaahn\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:581: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of groups for any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "# Defining the Logistic Regression\n",
    "l_reg = LogisticRegression()\n",
    "param_grid = {'penalty':['l1', 'l2']}\n",
    "gridlreg = GridSearchCV(l_reg , param_grid, cv=5 , return_train_score=True)\n",
    "logregclf = gridlreg.fit(X_train_scaled, y_train)\n",
    "# Prediction on Test Data \n",
    "lrp=logregclf.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaahn\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:581: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of groups for any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "# Defining the Random Forest model\n",
    "rf = RandomForestClassifier()\n",
    "param_grid = {'max_features':[5, 10, 20, 50, 100, 150, 200, 250,279]}\n",
    "rf = GridSearchCV(rf , param_grid, cv = 5 , return_train_score=True)\n",
    "rfclf = rf.fit(X_train_scaled, y_train)\n",
    "# Prediction on Test Data \n",
    "rfp=rfclf.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaahn\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:581: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of groups for any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "svc_linear_clf = LinearSVC(random_state = 10)\n",
    "param_grid = {'C':[0.01,0.1,1,3,5,10]}\n",
    "gridsvclinear=GridSearchCV(svc_linear_clf, param_grid,cv=5)\n",
    "svclinear = gridsvclinear.fit(X_train_scaled, y_train)\n",
    "# Prediction on Test Data \n",
    "svclinearp=svclinear.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVC Kernel Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaahn\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:581: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of groups for any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "svc_clf=SVC(kernel = 'linear')\n",
    "param_grid = {'C':[0.1,1,5,10,15]}\n",
    "gridsvc=GridSearchCV(svc_clf,param_grid,cv=5)\n",
    "svclm = gridsvc.fit(X_train_scaled,y_train)\n",
    "# Prediction on Test Data \n",
    "svc_linker_p=svclm.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVC RBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaahn\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:581: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of groups for any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC       \n",
    "svc_rbf_clf=SVC(kernel = 'rbf')\n",
    "param_grid = {'C':[0.1,1,5,10,15], 'gamma':[0.01, 1, 5]}\n",
    "gridsvcrbf=GridSearchCV(svc_rbf_clf,param_grid,cv=5)\n",
    "svcrbf = gridsvcrbf.fit(X_train_scaled,y_train)\n",
    "\n",
    "# Prediction on Test Data \n",
    "svcrbf_p=svcrbf.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVC Poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaahn\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:581: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of groups for any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC       \n",
    "svc_poly_clf=SVC(kernel = 'poly', degree = 3)\n",
    "param_grid = {'C':[0.1,1,5,10,15], 'gamma':[0.01, 1, 5]}\n",
    "gridsvcpoly=GridSearchCV(svc_poly_clf,param_grid,cv=5)\n",
    "svcpoly = gridsvcpoly.fit(X_train_scaled,y_train)\n",
    "\n",
    "# Prediction on Test Data \n",
    "svcpoly_p=svcpoly.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Best Score for KNN Classifier: 0.601769911504\n",
      "The Best Score for Decision Tree : 0.657817109145\n",
      "The Best Score for Logistic Regression : 0.693215339233\n",
      "The Best Score for Random Forest : 0.737463126844\n",
      "The Best Score for Linear SVM : 0.70796460177\n",
      "The Best Score for SVM with Linear Kernel : 0.70796460177\n",
      "The Best Score for SVM with RBF Kernel : 0.699115044248\n",
      "The Best Score for SVM with Poly Kernel : 0.660766961652\n"
     ]
    }
   ],
   "source": [
    "# Displaying the best scores of the models\n",
    "print(\"The Best Score for KNN Classifier:\", knnmodel.best_score_ )\n",
    "print(\"The Best Score for Decision Tree :\", dtclf.best_score_)\n",
    "print(\"The Best Score for Logistic Regression :\", logregclf.best_score_)\n",
    "print(\"The Best Score for Random Forest :\", rfclf.best_score_)\n",
    "print(\"The Best Score for Linear SVM :\", svclinear.best_score_)\n",
    "print(\"The Best Score for SVM with Linear Kernel :\", svclm.best_score_)\n",
    "print(\"The Best Score for SVM with RBF Kernel :\", svcrbf.best_score_)\n",
    "print(\"The Best Score for SVM with Poly Kernel :\", svcpoly.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying the Best Parameters for the models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Best Parameters for KNN Classifier: {'n_neighbors': 3}\n",
      "The Best Parameters for Decision Tree : {'max_depth': 5}\n",
      "The Best Parameters for Logistic Regression : {'penalty': 'l2'}\n",
      "The Best Parameters for Random Forest : {'max_features': 250}\n",
      "The Best Parameters for Linear SVM : {'C': 1}\n",
      "The Best Parameters for SVM with Linear Kernel : {'C': 1}\n",
      "The Best Parameters for SVM with RBF Kernel : {'C': 15, 'gamma': 0.01}\n",
      "The Best Parameters for SVM with Poly Kernel : {'C': 0.1, 'gamma': 1}\n"
     ]
    }
   ],
   "source": [
    "print(\"The Best Parameters for KNN Classifier:\", knnmodel.best_params_ )\n",
    "print(\"The Best Parameters for Decision Tree :\", dtclf.best_params_ )\n",
    "print(\"The Best Parameters for Logistic Regression :\", logregclf.best_params_)\n",
    "print(\"The Best Parameters for Random Forest :\", rfclf.best_params_)\n",
    "print(\"The Best Parameters for Linear SVM :\", svclinear.best_params_)\n",
    "print(\"The Best Parameters for SVM with Linear Kernel :\", svclm.best_params_)\n",
    "print(\"The Best Parameters for SVM with RBF Kernel :\", svcrbf.best_params_)\n",
    "print(\"The Best Parameters for SVM with Poly Kernel :\", svcpoly.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Displaying the Training and Testing Score of Models using Best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The definition of models with their best parameters\n",
    "knn_bp = KNeighborsClassifier(n_neighbors=3)\n",
    "lr_bp = LogisticRegression(penalty='l2',random_state=10)\n",
    "svclinear_bp=LinearSVC(C=1, random_state = 10)\n",
    "svclm_bp = SVC(kernel = 'linear', C=1,random_state=10)\n",
    "svcrbf_bp = SVC(kernel = 'rbf', C=15,gamma=0.01,random_state=10)\n",
    "svcpoly_bp = SVC(kernel = 'poly', degree = 3, C=0.1,gamma=1,random_state=10)\n",
    "dt_bp = DecisionTreeClassifier(max_depth = 5,random_state=10)\n",
    "rf_bp = RandomForestClassifier(max_features = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model details: KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=3, p=2,\n",
      "           weights='uniform')\n",
      "Training score of the model: 0.622418879056\n",
      "Testing score of the model: 0.46017699115\n",
      "\n",
      "\n",
      "Model details: LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=10, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "Training score of the model: 0.592920353982\n",
      "Testing score of the model: 0.548672566372\n",
      "\n",
      "\n",
      "Model details: LinearSVC(C=1, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=10, tol=0.0001,\n",
      "     verbose=0)\n",
      "Training score of the model: 0.834808259587\n",
      "Testing score of the model: 0.424778761062\n",
      "\n",
      "\n",
      "Model details: SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=False, random_state=10, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "Training score of the model: 0.666666666667\n",
      "Testing score of the model: 0.53982300885\n",
      "\n",
      "\n",
      "Model details: SVC(C=15, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.01, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=10, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "Training score of the model: 0.563421828909\n",
      "Testing score of the model: 0.557522123894\n",
      "\n",
      "\n",
      "Model details: SVC(C=0.1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=1, kernel='poly',\n",
      "  max_iter=-1, probability=False, random_state=10, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "Training score of the model: 1.0\n",
      "Testing score of the model: 0.424778761062\n",
      "\n",
      "\n",
      "Model details: DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=10, splitter='best')\n",
      "Training score of the model: 0.619469026549\n",
      "Testing score of the model: 0.477876106195\n",
      "\n",
      "\n",
      "Model details: RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=100, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)\n",
      "Training score of the model: 0.973451327434\n",
      "Testing score of the model: 0.513274336283\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models = [knn_bp,lr_bp,svclinear_bp,svclm_bp,svcrbf_bp,svcpoly_bp,dt_bp,rf_bp]\n",
    "for i in range(8):\n",
    "    models[i].fit(X_train_scaled,y_train)\n",
    "    print('Model details:',models[i])\n",
    "    print('Training score of the model:',models[i].score(X_train_scaled,y_train))\n",
    "    print('Testing score of the model:',models[i].score(X_test_scaled,y_test))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=3, p=2,\n",
      "           weights='uniform')\n",
      "training score: 0.587020648968\n",
      "testing score: 0.557522123894\n",
      "\n",
      "\n",
      "model: LogisticRegression(C=2, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=10, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "training score: 0.702064896755\n",
      "testing score: 0.592920353982\n",
      "\n",
      "\n",
      "model: LinearSVC(C=1, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=10, tol=0.0001,\n",
      "     verbose=0)\n",
      "training score: 0.941002949853\n",
      "testing score: 0.699115044248\n",
      "\n",
      "\n",
      "model: SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=False, random_state=10, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "training score: 0.654867256637\n",
      "testing score: 0.610619469027\n",
      "\n",
      "\n",
      "model: SVC(C=15, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.01, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=10, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "training score: 0.59587020649\n",
      "testing score: 0.557522123894\n",
      "\n",
      "\n",
      "model: SVC(C=0.1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=1, kernel='poly',\n",
      "  max_iter=-1, probability=False, random_state=10, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "training score: 0.65191740413\n",
      "testing score: 0.628318584071\n",
      "\n",
      "\n",
      "model: DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=10, splitter='best')\n",
      "training score: 0.82005899705\n",
      "testing score: 0.672566371681\n",
      "\n",
      "\n",
      "model: RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=100, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)\n",
      "training score: 0.790560471976\n",
      "testing score: 0.628318584071\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The list of models on which Bagging is performed\n",
    "model_list = [knn_bp,lr_bp,svclinear_bp,svclm_bp,svcrbf_bp,svcpoly_bp,dt_bp,rf_bp]\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# Running Bagging Classifier on all the models and displaying the results\n",
    "\n",
    "for m in model_list:\n",
    "    bag = BaggingClassifier(m,n_estimators=100,max_samples=100,bootstrap=True,n_jobs=-1,random_state=10)\n",
    "    bag.fit(X_train_scaled,y_train)\n",
    "    print('model:',m)\n",
    "    print('training score:',bag.score(X_train_scaled,y_train))\n",
    "    print('testing score:',bag.score(X_test_scaled,y_test))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By performing Bagging, there is no significant improvement in the accuracy of the models except for Logistic regression and Linear SVC and Decision Tree \n",
    "#### Note: Scaled Training Data set bagged with replacement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model details: LogisticRegression(C=2, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=10, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "Training score of model: 0.746312684366\n",
      "Testing score of model: 0.628318584071\n",
      "\n",
      "\n",
      "Model details: LinearSVC(C=1, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=10, tol=0.0001,\n",
      "     verbose=0)\n",
      "Training score of model: 0.941002949853\n",
      "Testing score of model: 0.699115044248\n",
      "\n",
      "\n",
      "Model details: SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=False, random_state=10, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "Training score of model: 0.542772861357\n",
      "Testing score of model: 0.53982300885\n",
      "\n",
      "\n",
      "Model details: SVC(C=15, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.01, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=10, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "Training score of model: 0.542772861357\n",
      "Testing score of model: 0.53982300885\n",
      "\n",
      "\n",
      "Model details: SVC(C=0.1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=1, kernel='poly',\n",
      "  max_iter=-1, probability=False, random_state=10, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "Training score of model: 1.0\n",
      "Testing score of model: 0.654867256637\n",
      "\n",
      "\n",
      "Model details: DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=10, splitter='best')\n",
      "Training score of model: 1.0\n",
      "Testing score of model: 0.663716814159\n",
      "\n",
      "\n",
      "Model details: RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=100, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)\n",
      "Training score of model: 1.0\n",
      "Testing score of model: 0.637168141593\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The list of models on which we are performing Adaboost\n",
    "m_list = [lr_bp,svclinear_bp,svclm_bp,svcrbf_bp,svcpoly_bp,dt_bp,rf_bp]\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Running AdaBoost on the models\n",
    "\n",
    "for model in m_list:\n",
    "    ada = AdaBoostClassifier(model,n_estimators=100,algorithm='SAMME',learning_rate=0.5,random_state=42)\n",
    "    ada.fit(X_train_scaled,y_train)\n",
    "    print('Model details:',model)\n",
    "    print('Training score of model:',ada.score(X_train_scaled,y_train))\n",
    "    print('Testing score of model:', ada.score(X_test_scaled,y_test))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using Ada Boost , there was no significant improvement in the accuracy of the model expect for Logistic Regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth: 1\n",
      "Learning rate: 0.001\n",
      "Training score: 0.557522123894\n",
      "Testing score: 0.548672566372\n",
      "\n",
      "\n",
      "Depth: 1\n",
      "Learning rate: 0.01\n",
      "Training score: 0.566371681416\n",
      "Testing score: 0.557522123894\n",
      "\n",
      "\n",
      "Depth: 1\n",
      "Learning rate: 0.1\n",
      "Training score: 0.784660766962\n",
      "Testing score: 0.504424778761\n",
      "\n",
      "\n",
      "Depth: 1\n",
      "Learning rate: 0.5\n",
      "Training score: 0.914454277286\n",
      "Testing score: 0.407079646018\n",
      "\n",
      "\n",
      "Depth: 2\n",
      "Learning rate: 0.001\n",
      "Training score: 0.572271386431\n",
      "Testing score: 0.548672566372\n",
      "\n",
      "\n",
      "Depth: 2\n",
      "Learning rate: 0.01\n",
      "Training score: 0.648967551622\n",
      "Testing score: 0.548672566372\n",
      "\n",
      "\n",
      "Depth: 2\n",
      "Learning rate: 0.1\n",
      "Training score: 1.0\n",
      "Testing score: 0.486725663717\n",
      "\n",
      "\n",
      "Depth: 2\n",
      "Learning rate: 0.5\n",
      "Training score: 1.0\n",
      "Testing score: 0.398230088496\n",
      "\n",
      "\n",
      "Depth: 3\n",
      "Learning rate: 0.001\n",
      "Training score: 0.648967551622\n",
      "Testing score: 0.53982300885\n",
      "\n",
      "\n",
      "Depth: 3\n",
      "Learning rate: 0.01\n",
      "Training score: 0.761061946903\n",
      "Testing score: 0.513274336283\n",
      "\n",
      "\n",
      "Depth: 3\n",
      "Learning rate: 0.1\n",
      "Training score: 1.0\n",
      "Testing score: 0.504424778761\n",
      "\n",
      "\n",
      "Depth: 3\n",
      "Learning rate: 0.5\n",
      "Training score: 1.0\n",
      "Testing score: 0.433628318584\n",
      "\n",
      "\n",
      "Depth: 4\n",
      "Learning rate: 0.001\n",
      "Training score: 0.70796460177\n",
      "Testing score: 0.530973451327\n",
      "\n",
      "\n",
      "Depth: 4\n",
      "Learning rate: 0.01\n",
      "Training score: 0.852507374631\n",
      "Testing score: 0.495575221239\n",
      "\n",
      "\n",
      "Depth: 4\n",
      "Learning rate: 0.1\n",
      "Training score: 1.0\n",
      "Testing score: 0.486725663717\n",
      "\n",
      "\n",
      "Depth: 4\n",
      "Learning rate: 0.5\n",
      "Training score: 1.0\n",
      "Testing score: 0.41592920354\n",
      "\n",
      "\n",
      "Depth: 5\n",
      "Learning rate: 0.001\n",
      "Training score: 0.737463126844\n",
      "Testing score: 0.513274336283\n",
      "\n",
      "\n",
      "Depth: 5\n",
      "Learning rate: 0.01\n",
      "Training score: 0.94395280236\n",
      "Testing score: 0.486725663717\n",
      "\n",
      "\n",
      "Depth: 5\n",
      "Learning rate: 0.1\n",
      "Training score: 1.0\n",
      "Testing score: 0.495575221239\n",
      "\n",
      "\n",
      "Depth: 5\n",
      "Learning rate: 0.5\n",
      "Training score: 1.0\n",
      "Testing score: 0.469026548673\n",
      "\n",
      "\n",
      "Depth: 6\n",
      "Learning rate: 0.001\n",
      "Training score: 0.781710914454\n",
      "Testing score: 0.513274336283\n",
      "\n",
      "\n",
      "Depth: 6\n",
      "Learning rate: 0.01\n",
      "Training score: 0.991150442478\n",
      "Testing score: 0.469026548673\n",
      "\n",
      "\n",
      "Depth: 6\n",
      "Learning rate: 0.1\n",
      "Training score: 1.0\n",
      "Testing score: 0.486725663717\n",
      "\n",
      "\n",
      "Depth: 6\n",
      "Learning rate: 0.5\n",
      "Training score: 1.0\n",
      "Testing score: 0.442477876106\n",
      "\n",
      "\n",
      "Depth: 7\n",
      "Learning rate: 0.001\n",
      "Training score: 0.83185840708\n",
      "Testing score: 0.504424778761\n",
      "\n",
      "\n",
      "Depth: 7\n",
      "Learning rate: 0.01\n",
      "Training score: 1.0\n",
      "Testing score: 0.424778761062\n",
      "\n",
      "\n",
      "Depth: 7\n",
      "Learning rate: 0.1\n",
      "Training score: 1.0\n",
      "Testing score: 0.451327433628\n",
      "\n",
      "\n",
      "Depth: 7\n",
      "Learning rate: 0.5\n",
      "Training score: 1.0\n",
      "Testing score: 0.486725663717\n",
      "\n",
      "\n",
      "Depth: 8\n",
      "Learning rate: 0.001\n",
      "Training score: 0.858407079646\n",
      "Testing score: 0.504424778761\n",
      "\n",
      "\n",
      "Depth: 8\n",
      "Learning rate: 0.01\n",
      "Training score: 1.0\n",
      "Testing score: 0.407079646018\n",
      "\n",
      "\n",
      "Depth: 8\n",
      "Learning rate: 0.1\n",
      "Training score: 1.0\n",
      "Testing score: 0.451327433628\n",
      "\n",
      "\n",
      "Depth: 8\n",
      "Learning rate: 0.5\n",
      "Training score: 1.0\n",
      "Testing score: 0.477876106195\n",
      "\n",
      "\n",
      "Depth: 9\n",
      "Learning rate: 0.001\n",
      "Training score: 0.879056047198\n",
      "Testing score: 0.495575221239\n",
      "\n",
      "\n",
      "Depth: 9\n",
      "Learning rate: 0.01\n",
      "Training score: 1.0\n",
      "Testing score: 0.424778761062\n",
      "\n",
      "\n",
      "Depth: 9\n",
      "Learning rate: 0.1\n",
      "Training score: 1.0\n",
      "Testing score: 0.451327433628\n",
      "\n",
      "\n",
      "Depth: 9\n",
      "Learning rate: 0.5\n",
      "Training score: 1.0\n",
      "Testing score: 0.41592920354\n",
      "\n",
      "\n",
      "Depth: 10\n",
      "Learning rate: 0.001\n",
      "Training score: 0.923303834808\n",
      "Testing score: 0.486725663717\n",
      "\n",
      "\n",
      "Depth: 10\n",
      "Learning rate: 0.01\n",
      "Training score: 1.0\n",
      "Testing score: 0.407079646018\n",
      "\n",
      "\n",
      "Depth: 10\n",
      "Learning rate: 0.1\n",
      "Training score: 1.0\n",
      "Testing score: 0.407079646018\n",
      "\n",
      "\n",
      "Depth: 10\n",
      "Learning rate: 0.5\n",
      "Training score: 1.0\n",
      "Testing score: 0.46017699115\n",
      "\n",
      "\n",
      "Depth: 11\n",
      "Learning rate: 0.001\n",
      "Training score: 0.941002949853\n",
      "Testing score: 0.495575221239\n",
      "\n",
      "\n",
      "Depth: 11\n",
      "Learning rate: 0.01\n",
      "Training score: 1.0\n",
      "Testing score: 0.353982300885\n",
      "\n",
      "\n",
      "Depth: 11\n",
      "Learning rate: 0.1\n",
      "Training score: 1.0\n",
      "Testing score: 0.371681415929\n",
      "\n",
      "\n",
      "Depth: 11\n",
      "Learning rate: 0.5\n",
      "Training score: 1.0\n",
      "Testing score: 0.469026548673\n",
      "\n",
      "\n",
      "Depth: 12\n",
      "Learning rate: 0.001\n",
      "Training score: 0.935103244838\n",
      "Testing score: 0.513274336283\n",
      "\n",
      "\n",
      "Depth: 12\n",
      "Learning rate: 0.01\n",
      "Training score: 1.0\n",
      "Testing score: 0.451327433628\n",
      "\n",
      "\n",
      "Depth: 12\n",
      "Learning rate: 0.1\n",
      "Training score: 1.0\n",
      "Testing score: 0.424778761062\n",
      "\n",
      "\n",
      "Depth: 12\n",
      "Learning rate: 0.5\n",
      "Training score: 1.0\n",
      "Testing score: 0.486725663717\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "for depth in range(1,10):\n",
    "    for rate in (0.001,0.01,0.1,0.5):\n",
    "        Grad_Boost = GradientBoostingClassifier(max_depth=depth,learning_rate=rate,random_state=10)\n",
    "        Grad_Boost.fit(X_train_scaled,y_train)\n",
    "        print('Depth:',depth)\n",
    "        print('Learning rate:',rate)\n",
    "        print('Training score:',Grad_Boost.score(X_train_scaled,y_train))\n",
    "        print('Testing score:',Grad_Boost.score(X_test_scaled,y_test))\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Gradient Boosting Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score: 0.896755162242\n",
      "Testing score: 0.646017699115\n"
     ]
    }
   ],
   "source": [
    "# Running the Gradient Boost with Best parameter \n",
    "Grad_Boost_best = GradientBoostingClassifier(max_depth=5,learning_rate=0.001,random_state=10)\n",
    "Grad_Boost_best.fit(X_train_scaled,y_train)\n",
    "\n",
    "#Printing the Training and Testing scores of the model\n",
    "print('Training score:',Grad_Boost_best.score(X_train_scaled,y_train))\n",
    "print('Testing score:',Grad_Boost_best.score(X_test_scaled,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting showed negative impact on decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.53466722e-01   1.00065478e-01   9.42096538e-02   8.45956035e-02\n",
      "   5.31203649e-02   5.06232770e-02   4.58521820e-02   4.11379222e-02\n",
      "   3.39555122e-02   2.91002953e-02   2.83069623e-02   2.24953053e-02\n",
      "   2.19076103e-02   1.93218640e-02   1.73115934e-02   1.53936622e-02\n",
      "   1.44518187e-02   1.22500906e-02   1.17679765e-02   9.14526871e-03\n",
      "   8.67874555e-03   7.66777844e-03   7.22818558e-03   6.69843001e-03\n",
      "   6.31079710e-03   5.84684795e-03   5.41859863e-03   5.27818860e-03\n",
      "   4.97710228e-03   4.78086850e-03   4.45784875e-03   4.19110783e-03\n",
      "   4.01413388e-03   3.73631418e-03   3.51610606e-03   3.27154304e-03\n",
      "   3.05061706e-03   2.99936504e-03   2.86983918e-03   2.53784205e-03\n",
      "   2.43258417e-03   2.30366770e-03   2.20400770e-03   2.16197256e-03\n",
      "   2.09370881e-03   1.98412559e-03   1.95335391e-03   1.72184367e-03\n",
      "   1.59578589e-03   1.45355598e-03   1.35403819e-03   1.27210776e-03\n",
      "   1.23770972e-03   1.19100740e-03   1.16412580e-03   1.07117385e-03\n",
      "   1.01509844e-03   9.94969245e-04   9.65581158e-04   8.98410860e-04\n",
      "   8.56455519e-04   7.85933931e-04   7.36128334e-04   6.55403435e-04\n",
      "   6.29896750e-04   6.08477639e-04   5.63838894e-04   5.39650905e-04\n",
      "   4.88147633e-04   4.41972049e-04   3.95608590e-04   3.71672586e-04\n",
      "   3.59390503e-04   3.48105158e-04   3.30408244e-04   3.04639550e-04\n",
      "   2.84000621e-04   2.78471896e-04   2.61375216e-04   2.40261768e-04\n",
      "   2.26969150e-04   2.18365608e-04   2.06665918e-04   1.93933421e-04\n",
      "   1.89774319e-04   1.72191185e-04   1.59231735e-04   1.50071859e-04\n",
      "   1.39145241e-04   1.33157318e-04   1.25979118e-04   1.18554287e-04\n",
      "   1.05035735e-04   9.70417059e-05   8.51292892e-05   8.25060508e-05\n",
      "   7.80533589e-05   7.06149253e-05   6.59013999e-05   6.02926795e-05\n",
      "   5.12025410e-05   5.01296316e-05   4.73938833e-05   4.44512717e-05\n",
      "   4.24635876e-05   4.13453066e-05   3.91114874e-05   3.68051581e-05\n",
      "   2.95109416e-05   2.91167761e-05   2.65119001e-05   2.47570138e-05\n",
      "   2.37582426e-05   2.13937334e-05   2.06153081e-05   1.84321554e-05\n",
      "   1.78447322e-05   1.59558786e-05   1.53617522e-05   1.26219315e-05\n",
      "   1.20271623e-05   1.05522083e-05   9.77769609e-06   9.57846503e-06\n",
      "   8.78131396e-06   7.98413204e-06   6.95247004e-06   6.24018945e-06\n",
      "   5.72811162e-06   5.33466490e-06   5.02906357e-06   4.23494574e-06\n",
      "   3.91966458e-06   3.50129694e-06   3.25399738e-06   3.10147974e-06\n",
      "   2.93682728e-06   2.76205514e-06   2.43525658e-06   2.40478977e-06\n",
      "   2.23054949e-06   2.11536706e-06   2.06049341e-06   1.85437739e-06\n",
      "   1.64809967e-06   1.61473849e-06   1.51729308e-06   1.44355209e-06\n",
      "   1.41628268e-06   1.26629292e-06]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(452, 150)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing the PCA function\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Defining the PCA\n",
    "pca = PCA(n_components = 150)\n",
    "pca_data = pca.fit_transform(Tdata)\n",
    "print(pca.explained_variance_ratio_)\n",
    "\n",
    "# Displaying the dimensions of PCA Data\n",
    "pca_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model details: KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=3, p=2,\n",
      "           weights='uniform')\n",
      "Training score of the model: 0.70796460177\n",
      "Testing score of the model: 0.53982300885\n",
      "\n",
      "\n",
      "Model details: LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=10, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "Training score of the model: 0.994100294985\n",
      "Testing score of the model: 0.575221238938\n",
      "\n",
      "\n",
      "Model details: LinearSVC(C=1, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=10, tol=0.0001,\n",
      "     verbose=0)\n",
      "Training score of the model: 0.911504424779\n",
      "Testing score of the model: 0.548672566372\n",
      "\n",
      "\n",
      "Model details: SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=False, random_state=10, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "Training score of the model: 1.0\n",
      "Testing score of the model: 0.646017699115\n",
      "\n",
      "\n",
      "Model details: SVC(C=15, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.01, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=10, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "Training score of the model: 1.0\n",
      "Testing score of the model: 0.557522123894\n",
      "\n",
      "\n",
      "Model details: SVC(C=0.1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=1, kernel='poly',\n",
      "  max_iter=-1, probability=False, random_state=10, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "Training score of the model: 1.0\n",
      "Testing score of the model: 0.548672566372\n",
      "\n",
      "\n",
      "Model details: DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=10, splitter='best')\n",
      "Training score of the model: 0.755162241888\n",
      "Testing score of the model: 0.58407079646\n",
      "\n",
      "\n",
      "Model details: RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=100, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)\n",
      "Training score of the model: 0.988200589971\n",
      "Testing score of the model: 0.530973451327\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Splitting the PCA Data into Train and Test Data\n",
    "pca_train,pca_test,y_train,y_test=train_test_split(pca_data,y_data,random_state=10)\n",
    "\n",
    "# Running the models on PCA Data\n",
    "models = [knn_bp,lr_bp,svclinear_bp,svclm_bp,svcrbf_bp,svcpoly_bp,dt_bp,rf_bp]\n",
    "for i in range(8):\n",
    "    models[i].fit(pca_train,y_train)\n",
    "    print('Model details:',models[i])\n",
    "    print('Training score of the model:',models[i].score(pca_train,y_train))\n",
    "    print('Testing score of the model:',models[i].score(pca_test,y_test))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA showed negative impact on all the models decreasing the accuracies of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation strategy considered was Accuracy. Considering all the results above, we can change the criteria to Recall or F-1 score."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
